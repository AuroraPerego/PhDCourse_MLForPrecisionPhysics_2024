{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "912290ce-bfeb-4089-a87d-cdefcb8a191e",
   "metadata": {},
   "source": [
    "# Improving the Neutrino conditional generation pre-training the Transformer encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9932271a-c1f3-4df9-9a9d-1af1a13d9f1e",
   "metadata": {},
   "source": [
    "The Neutrino generation normalizing flow is encoding the conditional probability $p(\\nu_1,\\nu_2 | reco event)$. The reco-level objects are analyzed by a transformer encoder model to create a fixed dimension tensor that is used to condition the normalizing flow transformations. \n",
    "\n",
    "In the notebook  **6_NeutrinoFlow** we have trained this conditional generative model by using just the maximum likelihood loss on the neutrinos from the MC training events. The transformer encoder is trained along the normalizing flow, without any special preparation. \n",
    "\n",
    "The performance of the model and the time (N. epochs) needed to make it converge can be greatly speed up if we pretrain the transformer encoder in order to make it maximally correlated with the object that we are trying to generate. \n",
    "\n",
    "A common strategy is to use a regression to make the transformer encoder output embegging, correlated explicitely with the neutrinos we are trying to generate.\n",
    "\n",
    "As an exercise, you will explore some possible implementations along these lines: \n",
    "\n",
    "1) Add a regression loss on the output of the transformer encoder, trying to model the neutrinos 4-momenta. This loss is added in parallel to the maximum likelihood flow loss, and the training is still optimizing both the encoder and the normalizing flow together from scratch. A small linear model (DNN) needs to be added on top of the transformer encoder output to regress the neutrino 4-momenta. Then the transformer encoder output is still passed to the normalizing flow to be used as the conditioning of the transformation layers. \n",
    "    - Experiment with different loss scales for the regression and max-likelihood loss.\n",
    "    - Add a MMD loss term to the regression\n",
    "    - (optinal) Try to use the mdmm constraint loss optimization to make the regression loss a \"constraint\" w.r.t of the max-likelihood loss.\n",
    "\n",
    "   \n",
    "2) Pre-train the transformer encoder + DNN for the regression, **before** the normalizing flow training. Save the weights of the transformer encoder and then load them in the encoder+normalizing flow original model. This strategy should speed up event more the generative model training.\n",
    "    - Train the normalizing flow with the maximum-likelihood loss, **freezing** the transformer encoder weights. Practically, the transformer encoder cannot change from the pre-trained status\n",
    "    - Train the normalizing flow with the maximum-likelihood loss, without freezing the transformer encoder weights. Check that the model is not diverging from the pre-trained minimum.\n",
    "    - (optional) Train the overall model like in the exercise 1, keeping in the loss a regression term. In this case the transformer encoder has been pretrained, so the regression term should just keep the encoder in the original minimum, or further optimize it.\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1535d852-4fe9-4129-8173-d3f24edccf0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
